{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b4buANVDgQ18",
    "outputId": "8ac36507-29c0-44ac-96ea-a59ca3a8b485"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes\n",
      "Accuracy: 0.7662337662337663\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.80      0.81        99\n",
      "           1       0.66      0.71      0.68        55\n",
      "\n",
      "    accuracy                           0.77       154\n",
      "   macro avg       0.75      0.75      0.75       154\n",
      "weighted avg       0.77      0.77      0.77       154\n",
      "\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Accuracy: 0.6428571428571429\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      1.00      0.78        99\n",
      "           1       0.00      0.00      0.00        55\n",
      "\n",
      "    accuracy                           0.64       154\n",
      "   macro avg       0.32      0.50      0.39       154\n",
      "weighted avg       0.41      0.64      0.50       154\n",
      "\n",
      "\n",
      "Bernoulli Naive Bayes\n",
      "Accuracy: 0.6558441558441559\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.99      0.79        99\n",
      "           1       0.75      0.05      0.10        55\n",
      "\n",
      "    accuracy                           0.66       154\n",
      "   macro avg       0.70      0.52      0.44       154\n",
      "weighted avg       0.69      0.66      0.54       154\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "\n",
    "# Split the dataset into features (X) and target (y)\n",
    "X = data.drop(columns=['Outcome'])\n",
    "y = data['Outcome']\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "'''\n",
    "Explanation:\n",
    "Why Use StandardScaler?\n",
    "Gaussian Naive Bayes assumes that features follow a Gaussian (normal) distribution. StandardScaler standardizes the dataset by centering the mean at 0 and scaling the variance to 1.\n",
    "Standardizing ensures that each feature's distribution aligns better with the Gaussian assumption, improving the algorithm's performance.\n",
    "Without standardization, features with larger scales might dominate the calculation of probabilities, violating the algorithm's assumptions.\n",
    "'''\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test)\n",
    "\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
    "print(classification_report(y_test, y_pred_gnb))\n",
    "\n",
    "# Multinomial Naive Bayes\n",
    "'''\n",
    "Why Use MinMaxScaler?\n",
    "Multinomial Naive Bayes assumes that features represent counts or frequencies. If the dataset contains features on widely different scales \n",
    "(e.g., one feature ranges from 0 to 1 while another ranges from 0 to 1000), it might skew the model.\n",
    "MinMaxScaler scales all feature values to a range of [0, 1], ensuring the features are proportional and normalized while preserving their relative differences.\n",
    "This step makes the data suitable for Multinomial Naive Bayes, which expects non-negative values and often benefits from normalized ranges.\n",
    "'''\n",
    "scaler_minmax = MinMaxScaler()\n",
    "X_minmax = scaler_minmax.fit_transform(X)\n",
    "X_train_mnb, X_test_mnb, y_train_mnb, y_test_mnb = train_test_split(X_minmax, y, test_size=0.2, random_state=42)\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_mnb, y_train_mnb)\n",
    "y_pred_mnb = mnb.predict(X_test_mnb)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_mnb, y_pred_mnb))\n",
    "print(classification_report(y_test_mnb, y_pred_mnb))\n",
    "\n",
    "# Bernoulli Naive Bayes\n",
    "'''\n",
    "Why Convert to Binary?\n",
    "Bernoulli Naive Bayes works with binary data where each feature indicates the presence (1) or absence (0) of a characteristic.\n",
    "The dataset X may contain continuous or count-based values, which do not fit Bernoulli Naive Bayes' assumption of binary features.\n",
    "The transformation (X > 0).astype(int) converts all positive values to 1 (presence) and all zero or negative values to 0 (absence), \n",
    "ensuring that the features comply with Bernoulli Naive Bayes' requirements.\n",
    "'''\n",
    "X_binary = (X > 0).astype(int)\n",
    "X_train_bnb, X_test_bnb, y_train_bnb, y_test_bnb = train_test_split(X_binary, y, test_size=0.2, random_state=42)\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train_bnb, y_train_bnb)\n",
    "y_pred_bnb = bnb.predict(X_test_bnb)\n",
    "\n",
    "print(\"\\nBernoulli Naive Bayes\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test_bnb, y_pred_bnb))\n",
    "print(classification_report(y_test_bnb, y_pred_bnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HgP0v7W9gpG1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
