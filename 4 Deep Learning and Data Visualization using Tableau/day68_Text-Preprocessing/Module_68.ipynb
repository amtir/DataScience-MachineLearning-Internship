{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtOVVPK0NUsn",
        "outputId": "6872862a-2e8f-4137-f008-f19bf2a16ef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize\n",
        "from nltk.util import ngrams\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word tokenize\n",
        "text = \"The sun is shining brightly\"\n",
        "tokens = word_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMAvZzuNx-Q",
        "outputId": "2dc90736-3a5f-43ad-a0a0-0637cb769000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'sun', 'is', 'shining', 'brightly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WordPunct Tokenization\n",
        "text = \"Hello! Howw yoo doin?\"\n",
        "tokens = WordPunctTokenizer().tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmlPWqfiOKgr",
        "outputId": "af0d500b-e9b2-4a8d-e2e7-3202fd3c39cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'Howw', 'yoo', 'doin', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sentence Tokenization\n",
        "text = 'She loves programming. She writes code.'\n",
        "tokens = sent_tokenize(text)\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ous7xLOfmA",
        "outputId": "311b3401-5f2f-4efc-88e4-000bc1fdcdaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['She loves programming.', 'She writes code.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unigrams\n",
        "text = \"She enjoys readings books\"\n",
        "tokens = word_tokenize(text)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "print(unigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns49JrOaOweX",
        "outputId": "158f467b-effb-492b-a5e2-fb7376846f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('She',), ('enjoys',), ('readings',), ('books',)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bigrams\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "print(bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7VLLFi4PH-h",
        "outputId": "a1511118-2aab-4fa9-bbe0-8f70d1398860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('She', 'enjoys'), ('enjoys', 'readings'), ('readings', 'books')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trigrams\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "print(trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JrR0D8JPXZK",
        "outputId": "c5f21091-751d-4c4f-e333-87d974e23dbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('She', 'enjoys', 'readings'), ('enjoys', 'readings', 'books')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#POS\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgFMdphBPbfF",
        "outputId": "e72a7b83-32bc-45fa-fbc9-7e090dea04a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"She enjoys reading books\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(pos_tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmBQhNByQqSt",
        "outputId": "06812f94-9ed1-44f4-a873-fcddd61f6d5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('She', 'PRP'), ('enjoys', 'VBZ'), ('reading', 'VBG'), ('books', 'NNS')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop words removal\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sM6WymBMQwkR",
        "outputId": "2709e67d-3202-4118-ef4e-fddc10b973ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"She enjoys reading books in library at college, punjab\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = word_tokenize(text)\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVQ4F-RBSO9y",
        "outputId": "780f18b9-4969-48f5-8e2b-648b465e4bb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['enjoys', 'reading', 'books', 'library', 'college', ',', 'punjab']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "bN9v4RCOSbzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The runners are running swiftly to catch the train. The happiness is evident in the smiles of the people.\"\n",
        "tokens = word_tokenize(text)\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4QdSWu2TbJt",
        "outputId": "51a8f718-5c67-4e11-fe00-d866df1ea901"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'runner', 'are', 'run', 'swiftli', 'to', 'catch', 'the', 'train', '.', 'the', 'happi', 'is', 'evid', 'in', 'the', 'smile', 'of', 'the', 'peopl', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "f5KEnGvJTqvc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f6746e-5f2c-45dc-9ed7-5140fc8ec805"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The children are playing with toys swiftly\"\n",
        "tokens = word_tokenize(text)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X4ivTUDTpcU",
        "outputId": "6d90b694-1ea0-4db9-9b1a-00769e00e232"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'child', 'are', 'playing', 'with', 'toy', 'swiftly']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NER\n",
        "from nltk import pos_tag, ne_chunk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('maxent_ne_chunker_tab')\n",
        "nltk.download('words')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLOFimZTT0FE",
        "outputId": "21759f35-e158-48eb-b485-ca820202da0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Anu works at Google and lives in Bengaluru. She visited Punjab in November 2025.\"\n",
        "tokens = word_tokenize(text)\n",
        "pos_tags = pos_tag(tokens)\n",
        "named_entities = ne_chunk(pos_tags)\n",
        "print(named_entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiUnu4jaVOmP",
        "outputId": "df2b493f-a1b1-4d4b-df2b-2a641ccb6522"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (GPE Anu/NNP)\n",
            "  works/VBZ\n",
            "  at/IN\n",
            "  (ORGANIZATION Google/NNP)\n",
            "  and/CC\n",
            "  lives/VBZ\n",
            "  in/IN\n",
            "  (GPE Bengaluru/NNP)\n",
            "  ./.\n",
            "  She/PRP\n",
            "  visited/VBD\n",
            "  (GPE Punjab/NNP)\n",
            "  in/IN\n",
            "  November/NNP\n",
            "  2025/CD\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tree import Tree\n",
        "entities = [(leaf[0],tree.label()) for tree in named_entities if isinstance(tree, Tree) for leaf in tree.leaves()]\n",
        "print(entities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97d83LhVViow",
        "outputId": "30b94bab-693a-444d-91a4-4b014090690a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Anu', 'GPE'), ('Google', 'ORGANIZATION'), ('Bengaluru', 'GPE'), ('Punjab', 'GPE')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#WSD\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJG6xE6KWW4j",
        "outputId": "b2cbceaa-d8b5-414a-fb5e-cccf2adad432"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'I went to bank of river'\n",
        "tokens = word_tokenize(text)\n",
        "sense = lesk(tokens,'bank')\n",
        "print(\"Word Sense : \",sense)\n",
        "print('Definition : ', sense.definition() if sense else 'No sense found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45x1y9qJX6an",
        "outputId": "0ef6cbea-24b2-48f5-f15b-95c0c9f2595a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Sense :  Synset('bank.v.07')\n",
            "Definition :  cover with ashes so to control the rate of burning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IlGSo_72YXQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}