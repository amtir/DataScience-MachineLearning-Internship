{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fedcb7e4-e712-424e-8e49-dd2216c76120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e79163-75e7-4c6a-a890-2c35f9f8edde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11128afc-1d5c-4844-bc5c-5ca5b205537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import  ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c429f22-d872-4c5c-a94e-c50014818a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8b1278a-eacb-475f-9076-f4ed25767b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc0d7409-0f5a-4f4c-81ff-57dde68c04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= \" The sun, sun. The Sun  is shining brightly?!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ef62058-8179-4922-afe0-8aed6e273f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'sun',\n",
       " ',',\n",
       " 'sun',\n",
       " '.',\n",
       " 'The',\n",
       " 'Sun',\n",
       " 'is',\n",
       " 'shining',\n",
       " 'brightly',\n",
       " '?',\n",
       " '!']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86c21fdc-afc2-4e99-b7c5-3761cd36fb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'sun',\n",
       " ',',\n",
       " 'sun',\n",
       " '.',\n",
       " 'The',\n",
       " 'Sun',\n",
       " 'is',\n",
       " 'shining',\n",
       " 'brightly',\n",
       " '?!']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "15578874-b8af-4cf6-bc6a-96cad75072d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' The sun, sun.', 'The Sun  is shining brightly?', '!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = sent_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77d1547b-51df-42f4-ba42-96150189e071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'sun', 'is', 'shining', 'brightly']\n"
     ]
    }
   ],
   "source": [
    "#Word tokenize\n",
    "text = \"The sun is shining brightly\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55d20b58-bacf-48b7-8566-7d03ad7d9f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'Howw', 'yoo', 'doin', '?']\n"
     ]
    }
   ],
   "source": [
    "#WordPunct Tokenization\n",
    "text = \"Hello! Howw yoo doin?\"\n",
    "tokens = WordPunctTokenizer().tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "88ec95c2-a45f-4ed6-9ceb-ca627659e0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['She loves programming.', 'She writes code.']\n"
     ]
    }
   ],
   "source": [
    "#Sentence Tokenization\n",
    "text = 'She loves programming. She writes code.'\n",
    "tokens = sent_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9076176e-18e3-483d-b83d-b3d9dda67182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She',), ('enjoys',), ('readings',), ('books',)]\n"
     ]
    }
   ],
   "source": [
    "#Unigrams\n",
    "text = \"She enjoys readings books\"\n",
    "tokens = word_tokenize(text)\n",
    "unigrams = list(ngrams(tokens, 1))\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a91216af-c10d-46b2-96a5-1b7e8cabb994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8034f51-4f7c-4ed9-b429-11649917040a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('She', 'PRP'), ('enjoys', 'VBZ'), ('reading', 'VBG'), ('books', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "text = \"She enjoys reading books\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "001b411e-2ffa-45af-8b8c-77015485db84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stop words removal\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af1ce6ed-31b0-4ece-8de1-029aec1e7715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['enjoys', 'reading', 'books', 'library', 'college', ',', 'punjab', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"She enjoys reading books in library at college, punjab!\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "tokens = word_tokenize(text)\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aac730e-48e7-4ce3-a2ba-588ff97a1ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39163e5b-1342-4222-ae07-fe1898cdb4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'runner', 'are', 'run', 'swiftli', 'to', 'catch', 'the', 'train', '.', 'the', 'happi', 'is', 'evid', 'in', 'the', 'smile', 'of', 'the', 'peopl', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"The runners are running swiftly to catch the train. The happiness is evident in the smiles of the people.\"\n",
    "tokens = word_tokenize(text)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece965c1-6441-4b76-8225-968c72733a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afec4d80-c354-4b7f-9597-99b248eba1d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d730140e-3318-4760-a1de-5270508b06e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'child', 'are', 'playing', 'with', 'toy', 'swiftly']\n"
     ]
    }
   ],
   "source": [
    "text = \"The children are playing with toys swiftly\"\n",
    "tokens = word_tokenize(text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94874fc6-eeed-4a1c-8fa5-200ff6922dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\akram\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag, ne_chunk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "447f3101-d9e4-4a00-9bf5-8662dbf94204",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Anu/NNP)\n",
      "  works/VBZ\n",
      "  at/IN\n",
      "  (ORGANIZATION Google/NNP)\n",
      "  and/CC\n",
      "  lives/VBZ\n",
      "  in/IN\n",
      "  (GPE Bengaluru/NNP)\n",
      "  ./.\n",
      "  She/PRP\n",
      "  visited/VBD\n",
      "  (GPE Punjab/NNP)\n",
      "  in/IN\n",
      "  November/NNP\n",
      "  2025/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "text = \"Anu works at Google and lives in Bengaluru. She visited Punjab in November 2025.\"\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "named_entities = ne_chunk(pos_tags)\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5182a0f-36c7-452b-8842-294651bf7f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming:\n",
      "run\n",
      "fli\n",
      "better\n",
      "\n",
      "Lemmatization:\n",
      "run\n",
      "fly\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "word1 = \"running\"\n",
    "word2 = \"flies\"\n",
    "word3 = \"better\"\n",
    "\n",
    "print(\"Stemming:\")\n",
    "print(stemmer.stem(word1))  # run\n",
    "print(stemmer.stem(word2))  # fli\n",
    "print(stemmer.stem(word3))  # better\n",
    "\n",
    "print(\"\\nLemmatization:\")\n",
    "print(lemmatizer.lemmatize(word1, pos=\"v\"))  # run\n",
    "print(lemmatizer.lemmatize(word2, pos=\"v\"))  # fly\n",
    "print(lemmatizer.lemmatize(word3, pos=\"a\"))  # good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2fd4f2e-100f-4d1e-aacb-29f7f60fa637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Anu', 'GPE'), ('Google', 'ORGANIZATION'), ('Bengaluru', 'GPE'), ('Punjab', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "entities = [(leaf[0],tree.label()) for tree in named_entities if isinstance(tree, Tree) for leaf in tree.leaves()]\n",
    "print(entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab4f704b-bb07-4144-b81c-64769e32eba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense in Sentence 1: a person who is of equal standing with another in a group\n",
      "Sense in Sentence 2: the score needed to win a match\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "sentence1 = \"She is looking for a match.\"\n",
    "sentence2 = \"Yesterdayâ€™s football match was exciting.\"\n",
    "\n",
    "# Applying WSD\n",
    "sense1 = lesk(sentence1.split(), \"match\")\n",
    "sense2 = lesk(sentence2.split(), \"match\")\n",
    "\n",
    "print(f\"Sense in Sentence 1: {sense1.definition()}\")\n",
    "print(f\"Sense in Sentence 2: {sense2.definition()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0c1dfe8-0365-476e-87ba-34a7e7dff5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Sense :  Synset('savings_bank.n.02')\n",
      "Definition :  a container (usually with a slot in the top) for keeping money at home\n"
     ]
    }
   ],
   "source": [
    "text = 'I went to bank to deposit some money'\n",
    "tokens = word_tokenize(text)\n",
    "sense = lesk(tokens,'bank')\n",
    "print(\"Word Sense : \",sense)\n",
    "print('Definition : ', sense.definition() if sense else 'No sense found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2027bf4e-479b-4872-8650-0a87539be3b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
